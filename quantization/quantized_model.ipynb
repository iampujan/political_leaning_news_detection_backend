{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xnhyx7xTT2wT"
      },
      "source": [
        "# Utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDeZ4g4LL5C6"
      },
      "outputs": [],
      "source": [
        "!pip install -q gdown\n",
        "!pip install quanto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5I5WE8XNswM"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder\n",
        "import warnings\n",
        "import string\n",
        "import torch\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW\n",
        "from transformers import get_scheduler\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import os\n",
        "import torch.quantization as quant"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_size_of_model(model):\n",
        "    torch.save(model.state_dict(), \"temp.p\")\n",
        "    size_mb = os.path.getsize(\"temp.p\") / 1e6\n",
        "    os.remove(\"temp.p\")\n",
        "    return size_mb"
      ],
      "metadata": {
        "id": "E0Cqp5_sStac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGCvS1Pe-1ji"
      },
      "source": [
        "# Pre-Trained Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N--Oo__d_nLK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e5b1781-3a78-4253-db35-e8ef2e0de718"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model state\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r03BlVcmSk2P",
        "outputId": "842c8763-056e-4941-e169-59b59871b660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-72d2fab971ac>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('model.pt'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print fine-tuned model size\n",
        "model_size = print_size_of_model(model)\n",
        "print(f\"Quantized model size: {model_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJvui-u2TCE1",
        "outputId": "14d2b2c2-8ebe-43dd-ae94-a92f77811e92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model size: 438.00 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J24_wDEqoejn"
      },
      "source": [
        "# Post Training Quantization (PTQ)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply dynamic quantization\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
        ")"
      ],
      "metadata": {
        "id": "Lgw4QWNmRW_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print quantized model size\n",
        "quantized_size = print_size_of_model(quantized_model)\n",
        "print(f\"Quantized model size: {quantized_size:.2f} MB\")"
      ],
      "metadata": {
        "id": "eLaZISc1RZPF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92fd688d-580c-4c88-854f-de07c011f49c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized model size: 181.48 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'quantized_model.pt')"
      ],
      "metadata": {
        "id": "xpJNoN6yi5aZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('quantized_model.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "lDdn0y3sWvmy",
        "outputId": "2fe2dc0a-a3b2-45ac-f595-ee4319da54cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_45ee7a28-38f7-424f-93d8-beeae04af815\", \"quantized_model.pt\", 438018632)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVQelLFbgkGE"
      },
      "source": [
        "# Real Life Example"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the Mode"
      ],
      "metadata": {
        "id": "-qJ0ZwW08rmu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBbwH91tc-oN"
      },
      "outputs": [],
      "source": [
        "headline = 'For Russia, Nuclear Weapons Are the Ultimate Bargaining Chip'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TubcdHpddEox"
      },
      "outputs": [],
      "source": [
        "body = \"\"\"On the 1,000th day of the war in Ukraine, President Volodymyr Zelensky took advantage of Washington’s new willingness to allow long-range missiles to be shot deep into Russia. Until this weekend, President Biden had declined to allow such strikes using American weapons, out of fear they could prompt World War III.\n",
        "\n",
        "On the same day, Russia formally announced a new nuclear doctrine that it had signaled two months ago, declaring for the first time that it would use nuclear weapons not only in response to an attack that threatened its survival, but also in response to any attack that posed a “critical threat” to its sovereignty and territorial integrity — a situation very similar to what was playing out in the Kursk region, as American-made ballistic missiles struck Russian weapons arsenals.\n",
        "\n",
        "And there was another wrinkle to Russia’s guidelines for nuclear use: For the first time, it declared the right to use nuclear weapons against a state that only possesses conventional arms — if it is backed by a nuclear power. Ukraine, backed by the United States, Britain and France — three of the five original nuclear-armed states — seems to be the country Russia’s president, Vladimir V. Putin, had in mind.\n",
        "\n",
        "Yet it was telling that the reaction in Washington on Tuesday was just short of a yawn. Officials dismissed the doctrine as the nothingburger of nuclear threats. Instead, the city was rife with speculation over who would prevail as Treasury secretary, or whether Matt Gaetz, a former congressman surrounded by sex-and-drug allegations though never charged, could survive the confirmation process to become attorney general.\n",
        "\n",
        "The Ukraine war has changed many things: It has ended hundreds of thousands of lives and shattered millions, it has shaken Europe, and it has deepened the enmity between Russia and the United States. But it has also inured Washington and the world to the renewed use of nuclear weapons as the ultimate bargaining chip. The idea that one of the nine countries now in possession of nuclear weapons — with Iran on the threshold of becoming the tenth — might press the button is more likely to evoke shrugs than a convening of the United Nations Security Council.\n",
        "\n",
        "“This is a signaling exercise, trying to scare audiences in Europe — and to a lesser extent, the United States — into falling off support for Ukraine,” said Matthew Bunn, a Harvard professor who has tracked nuclear risks for decades. “The actual short-term probability of Russian nuclear use hasn’t increased. The long-term probability of nuclear war has probably increased slightly — because U.S. willingness to support strikes deep into Russia is reinforcing Putin’s hatred and fear of the West, and will likely provoke Russian responses that will increase Western fear and hatred of Russia.”\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zfbJmGJSd2Ya"
      },
      "outputs": [],
      "source": [
        "data = [f'{headline} {body}']\n",
        "encodings = tokenizer(data, truncation=True, padding=True, max_length=128, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y3oNq6UsfbgN"
      },
      "outputs": [],
      "source": [
        "# Move model to the correct device\n",
        "quantized_model.to('cpu')\n",
        "quantized_model.eval()\n",
        "\n",
        "# Move input tensors (encodings) to the same device\n",
        "encodings = {key: val.to('cpu') for key, val in encodings.items()}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = quantized_model(**encodings)  # No need to call .to(device) on outputs\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Get predictions\n",
        "    predicted_class = torch.argmax(logits, dim=-1)\n",
        "    predictions.extend(predicted_class.cpu().numpy())  # Move predictions to CPU for numpy compatibility\n",
        "\n",
        "print(\"Predictions:\", predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhyCIAeUVk7U",
        "outputId": "af8d03f9-607f-430b-a470-bfe80019067d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: [2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tsk69KIz-DkR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9108e0-c574-43c0-e21a-c65797522e74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[28.2651, 31.6096, 40.1253]])\n"
          ]
        }
      ],
      "source": [
        "probabilities = F.softmax(logits, dim=1)\n",
        "percentages = probabilities * 100\n",
        "print(percentages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq1RYgiUfskX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3643c8b9-8fd7-45d8-a5c3-c75505b9828f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "predictions"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bj8g0FU6Wtlc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}